---
title: "What to do with an odds ratio of 35?"
title-slide-attributes:
    data-background-image: "https://images.unsplash.com/photo-1464802686167-b939a6910659?q=80&w=3250&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
bibliography: policing_the_pandemic.bib
author: "Dr. Ben Matthews | University of Stirling"
format:
  revealjs:
    smaller: true
    scrollable: true
    theme: default
editor: visual
---

## Before we begin

-   This is *extremely* work-in-progress!
-   Any and all comments much appreciated

## Background

-   This question came from the [Policing the Pandemic in Scotland](https://www.law.ed.ac.uk/research/research-projects/policing-the-pandemic) project with excellent colleagues [Dr Vicky Gorton](https://www.law.ed.ac.uk/people/dr-victoria-gorton) and [Prof Susan McVie](https://www.law.ed.ac.uk/people/professor-susan-mcvie)
-   Vicky has been leading on our linked data analysis, which has brought together data on all people in Scotland who received a FPN for breaching the covid regulations between X and Y with information on their health and (some) social circumstances, as well as a comparison group of matched controls
-   We want to know if people with more 'vulnerability' (read - health service use) are more likely than others to have received a Covid FPN

## Case-controls and conditional logistic regression

-   Because we have a case-control study design (all people we know received a FPN and matched controls), we used conditional logistic regression [@gailLikelihoodCalculationsMatched1981]
-   (Although some people say this is the wrong method because of course they do [@kuoUnconditionalConditionalLogistic2018])
-   We had **three** controls for each case, and around 18,000 FPNs with a match rate around 85%

## Case-controls and conditional logistic regression

-   One downside of the case-control design (and associated methods) is that the intercept in your regression model is basically meaningless (`clogit` in Stata [doesn't even report it](https://www.stata.com/manuals/rclogit.pdf))

-   This is because the model's intercept reflects the prevalence of your outcome in your data for your reference category

-   With a case-control design, the prevalence of the outcome is set by the study design - with three controls for every case the prevalence of the outcome for the whole study is going to be 25%. This tells us nothing useful!

## Case-controls and conditional logistic regression

-   So we (as is recommended) default to using Odds Ratios to understand the association between service use and receiving a FPN

-   However, some of our odds ratios are very high (up to 35!) (specifically for the association between multiple contacts with health services and FPN receipt in lockdown one)

-   If we were using regular logistic regression with a general population sample then we could happily calculate marginal effects of our key variables, present them as differences in the probability of receiving a FPN and get on with our lives

-   ... but we can't do this

## So what now?

-   So we've been puzzling about what we *can* do. What follows is the result of going down this rabbit-hole

-   What follows is only loosely related to our actual project!

## Describe 'em all

-   What we want to do is describe our Odds Ratios as differences in probabilities

-   What we can do is describe *all* the differences in probabilities that are consistent with a given Odds Ratio

-   This is just maths - for a given Odds Ratio and a fixed 'reference' probability, we can calculate the difference in probabilities between comparison and reference categories

-   So if we just take a bunch of reference probabilities and Odds Ratios we can calculate the difference in probabilities that the odds ratios describe

## Probabilities

::: columns
::: {.column width="70%"}
![](03_figures/merlo_plot.png){fig-alt="Insert alt text."}
:::

::: {.column width="30%"}
:::
:::




## Difference in probabilities

::: columns
::: {.column width="70%"}
![](03_figures/diff_plot.png){fig-alt="Insert alt text."}
:::

::: {.column width="30%"}
_This is the same data as the previous plot but showing the difference probabilities, not the raw probabilities_
:::
:::
 

## Example plot


::: columns
::: {.column width="70%"}
![](03_figures/sample_plot.png){fig-alt="Insert alt text."}
:::

::: {.column width="30%"}
:::
:::



## The Universe of marginal effects

-   Every marginal effect for an Odds Ratio of 35 is on this line

-   Aside: This is also a nice way of thinking about different flavours of marginal effect

## MEMs
::: columns
::: {.column width="70%"}
![](03_figures/mem_plot.png){fig-alt="Insert alt text."}
:::

::: {.column width="30%"}
_For Marginal Effects at the Mean (MEMs) you in effect pick a single point on this curve_
:::
:::




## MERs

::: columns
::: {.column width="70%"}
![](03_figures/mer_plot.png){fig-alt="Insert alt text."}
:::

::: {.column width="30%"}
_For Marginal Effects at Representative values (MERs) you are picking a few point on this curve_
:::
:::


## AMEs

::: columns
::: {.column width="70%"}
![](03_figures/ame_plot.png){fig-alt="Insert alt text."}
:::

::: {.column width="30%"}
_For Average Marginal Effects you get a point on this curve for every person/row in your dataset and then you take the average_
:::
:::


## Back to our data

::: columns
::: {.column width="70%"}
![](03_figures/our_dat_plot.png) fig-alt="Insert alt text."}
:::

::: {.column width="30%"}
- Very approximately, our Odds Ratio of 35 implies a difference in probabilities of around 66%!
-  Given that we know the refrence probability is around 25% from the study design, for our study approximately 92% of people who had a FPN during lockdown one had 3 or more health conditions?
:::
:::



## Back to our data

-   increase in prob from prob of having an fpn with ! 3 or more health conditions

-   so n_fpn !3+ health / n_fpn 3+ health + n_ctrl 3+ health

-   compared to n_fpn 3+ health / n_fpn 3+ health + n_ctrl 3+health

## We know things

-   One helpful thought from framing marginal effects as picking points off this line is that it emphasises that you can decide which population you want to describe your results as being 'marginalized' over

-   We know that a difference in probabilities in a world where a quarter of all people in Scotland were fined for breaching Covid rules is silly

-   In fact, we know pretty precisely that the prevalence of getting a FPN in lockdown one for the whole population was around 0.001% [@mcvieThirdDataReport2021], not 25%

## Our plot again

::: columns
::: {.column width="70%"}
![](03_figures/our_dat_plot2.png) fig-alt="Insert alt text."}
:::

::: {.column width="30%"}
-   This is an increase in probabilities of closer to 1.5 percentage points not 66

- But now I've done this I'm not sure it's told us anything that useful?
:::
:::



## What did I learn?

-   From just an Odds Ratio you can figure out all the possible marginal effects consistent with this Odds Ratio

-   If you have additional information (like we did) this means you can manually come up with a guesstimate of a marginal effect for a value/range of values that you care about, even if the original analysis/model doesn't give you this

- Not AMEs though, and I'm not sure about confidence intervals

## Was all this pointless?

-   Having done all this, I actually don't think we'll use this in our paper. Thinking hard about the population that we're interested in made me wonder..

-   ... and what's wrong with an odds ratio of 35 anyway?

-   This is an accurate description of our dataset! (No need for confidence intervals here?)

-   If the problem is that we don't think a result this extreme would generalize to another 'sample' from the sample population - with close to every person who received an FPN do we even have any issues of generalizability (we have basically 100% of the relevant people)?

## Was all this pointless?

-   Instead of generalizability, I think we have either a massive issue with transportability/external validity [@degtiarReviewGeneralizabilityTransportability2023], or we have no issue at all

-   It seems nonsensical to suggest that these results would apply to another country during Covid or another pandemic (countries were [very different in their responses](https://ourworldindata.org/policy-responses-covid))

-   The results for lockdown one in Scotland don't even generalize to lockdown two - we show that in our analysis!



## Uncertainty in observational data

-   This might be pretty unique to studying Covid? 

- And does this means that we don't really need confidence intervals here? Common logic from other contexts doesn't seem to apply:

-   In the context of US congressional elections:

> We would've got the wrong answer if we had tried to get uncertainties for our estimates \[of electoral results under different 'swing' patterns\] by "bootstrapping" the 435 congressional elections. We wanted inferences for these 435 under hypothetical alternative conditions, not inference for the entire population or for another sample of 435.

- See [@gelmanHowYouInterpret2011]


## Uncertainty in observational data

- Maybe we care about uncertainty in our inferences more in the sense that a small number of cases may be really affecting our results (like Approximate Maximum Influence Perturbation [@broderickAutomaticFiniteSampleRobustness2023])? But even these kinds of measures are framed as being about differences between sample and population

- Perhaps we should be worried about influence of non-matches in the data linkage? More than 10% of cases didn't match so maybe this is worth considering

- Not clear what we should do about that though!

## Conclusions

-   We had some large Odds Ratios

-   I tried to figure out how we could best interpret them

-   In the end the best way was to say 'hey we had some large Odds Ratios'

## Thank you! {.smaller}

::: {#refs}
:::
