@misc{broderickAutomaticFiniteSampleRobustness2023,
  title = {An {{Automatic Finite-Sample Robustness Metric}}: {{When Can Dropping}} a {{Little Data Make}} a {{Big Difference}}?},
  shorttitle = {An {{Automatic Finite-Sample Robustness Metric}}},
  author = {Broderick, Tamara and Giordano, Ryan and Meager, Rachael},
  year = {2023},
  month = jul,
  number = {arXiv:2011.14999},
  eprint = {2011.14999},
  primaryclass = {econ, stat},
  publisher = {{arXiv}},
  urldate = {2023-11-30},
  abstract = {Study samples often differ from the target populations of inference and policy decisions in non-random ways. Researchers typically believe that such departures from random sampling -- due to changes in the population over time and space, or difficulties in sampling truly randomly -- are small, and their corresponding impact on the inference should be small as well. We might therefore be concerned if the conclusions of our studies are excessively sensitive to a very small proportion of our sample data. We propose a method to assess the sensitivity of applied econometric conclusions to the removal of a small fraction of the sample. Manually checking the influence of all possible small subsets is computationally infeasible, so we use an approximation to find the most influential subset. Our metric, the "Approximate Maximum Influence Perturbation," is based on the classical influence function, and is automatically computable for common methods including (but not limited to) OLS, IV, MLE, GMM, and variational Bayes. We provide finite-sample error bounds on approximation performance. At minimal extra cost, we provide an exact finite-sample lower bound on sensitivity. We find that sensitivity is driven by a signal-to-noise ratio in the inference problem, is not reflected in standard errors, does not disappear asymptotically, and is not due to misspecification. While some empirical applications are robust, results of several influential economics papers can be overturned by removing less than 1\% of the sample.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  file = {/home/work/Zotero/storage/BLJP5P4S/Broderick et al. - 2023 - An Automatic Finite-Sample Robustness Metric When.pdf;/home/work/Zotero/storage/3BRAT9AQ/2011.html}
}

@article{degtiarReviewGeneralizabilityTransportability2023,
  title = {A {{Review}} of {{Generalizability}} and {{Transportability}}},
  author = {Degtiar, Irina and Rose, Sherri},
  year = {2023},
  journal = {Annual Review of Statistics and Its Application},
  volume = {10},
  number = {1},
  pages = {501--524},
  doi = {10.1146/annurev-statistics-042522-103837},
  urldate = {2023-11-24},
  abstract = {When assessing causal effects, determining the target population to which the results are intended to generalize is a critical decision. Randomized and observational studies each have strengths and limitations for estimating causal effects in a target population. Estimates from randomized data may have internal validity but are often not representative of the target population. Observational data may better reflect the target population, and hence be more likely to have external validity, but are subject to potential bias due to unmeasured confounding. While much of the causal inference literature has focused on addressing internal validity bias, both internal and external validity are necessary for unbiased estimates in a target population. This article presents a framework for addressing external validity bias, including a synthesis of approaches for generalizability and transportability, and the assumptions they require, as well as tests for the heterogeneity of treatment effects and differences between study and target populations.},
  keywords = {causal inference,external validity,generalizability,transportability,treatment effect heterogeneity},
  file = {/home/work/Zotero/storage/MJRB2MJL/Degtiar and Rose - 2023 - A Review of Generalizability and Transportability.pdf}
}

@article{gailLikelihoodCalculationsMatched1981,
  title = {Likelihood Calculations for Matched Case-Control Studies and Survival Studies with Tied Death Times},
  author = {Gail, Mitchell H. and Lubin, Jay H. and Rubinstein, Larence V.},
  year = {1981},
  month = dec,
  journal = {Biometrika},
  volume = {68},
  number = {3},
  pages = {703--707},
  issn = {0006-3444},
  doi = {10.1093/biomet/68.3.703},
  urldate = {2023-11-28},
  abstract = {A recursion is presented which permits rapid computation of a conditional likelihood function which arises in matched case-control studies and survival studies with tied death times. An example with seven parameters and ten large strata, the largest of which contains 29 cases and 52 controls, illustrates the feasibility of the conditional maximum likelihood solution. These methods may be specialized to permit rapid calculation of the conditional maximum likelihood estimate of the common odds ratio for several 2{\texttimes}2 tables.},
  file = {/home/work/Zotero/storage/6BWFD66G/218786.html}
}

@misc{gelmanHowYouInterpret2011,
  title = {How Do You Interpret Standard Errors from a Regression Fit to the Entire Population?},
  author = {Gelman, King},
  year = {2011},
  month = oct,
  journal = {Statistical Modeling, Causal Inference, and Social Science}
}

@article{gelmanUnifiedMethodEvaluating1994,
  title = {A {{Unified Method}} of {{Evaluating Electoral Systems}} and {{Redistricting Plans}}},
  author = {Gelman, Andrew and King, Gary},
  year = {1994},
  journal = {American Journal of Political Science},
  volume = {38},
  number = {2},
  eprint = {2111417},
  eprinttype = {jstor},
  pages = {514--554},
  publisher = {{[Midwest Political Science Association, Wiley]}},
  issn = {0092-5853},
  doi = {10.2307/2111417},
  urldate = {2023-11-28},
  abstract = {We derive a unified statistical method with which one can produce substantially improved definitions and estimates of almost any feature of two-party electoral systems that can be defined based on district vote shares. Our single method enables one to calculate more efficient estimates, with more trustworthy assessments of their uncertainty, than each of the separate multifarious existing measures of partisan bias, electoral responsiveness, seats-votes curves, expected or predicted vote in each district in a legislature, the probability that a given party will win the seat in each district, the proportion of incumbents or others who will lose their seats, the proportion of women or minority candidates to be elected, the incumbency advantage and other causal effects, the likely effects on the electoral system and district votes of proposed electoral reforms such as term limitations, campaign spending limits, and drawing majority-minority districts, and numerous others. To illustrate, we estimate the partisan bias and electoral responsiveness of the U.S. House of Representatives since 1900 and evaluate the fairness of competing redistricting plans for the 1992 Ohio state legislature.},
  file = {/home/work/Zotero/storage/JNIHGBAI/Gelman and King - 1994 - A Unified Method of Evaluating Electoral Systems a.pdf}
}

@article{kuoUnconditionalConditionalLogistic2018,
  title = {Unconditional or {{Conditional Logistic Regression Model}} for {{Age-Matched Case}}{\textendash}{{Control Data}}?},
  author = {Kuo, Chia-Ling and Duan, Yinghui and Grady, James},
  year = {2018},
  journal = {Frontiers in Public Health},
  volume = {6},
  issn = {2296-2565},
  urldate = {2023-11-24},
  abstract = {Matching on demographic variables is commonly used in case{\textendash}control studies to adjust for confounding at the design stage. There is a presumption that matched data need to be analyzed by matched methods. Conditional logistic regression has become a standard for matched case{\textendash}control data to tackle the sparse data problem. The sparse data problem, however, may not be a concern for loose-matching data when the matching between cases and controls is not unique, and one case can be matched to other controls without substantially changing the association. Data matched on a few demographic variables are clearly loose-matching data, and we hypothesize that unconditional logistic regression is a proper method to perform. To address the hypothesis, we compare unconditional and conditional logistic regression models by precision in estimates and hypothesis testing using simulated matched case{\textendash}control data. Our results support our hypothesis; however, the unconditional model is not as robust as the conditional model to the matching distortion that the matching process not only makes cases and controls similar for matching variables but also for the exposure status. When the study design involves other complex features or the computational burden is high, matching in loose-matching data can be ignored for negligible loss in testing and estimation if the distributions of matching variables are not extremely different between cases and controls.},
  file = {/home/work/Zotero/storage/P6XCNI5U/Kuo et al. - 2018 - Unconditional or Conditional Logistic Regression M.pdf}
}

@article{lewallenEpidemiologyPracticeCaseControl1998,
  title = {Epidemiology in {{Practice}}: {{Case-Control Studies}}},
  shorttitle = {Epidemiology in {{Practice}}},
  author = {Lewallen, Susan and Courtright, Paul},
  year = {1998},
  journal = {Community Eye Health},
  volume = {11},
  number = {28},
  pages = {57--58},
  issn = {0953-6833},
  urldate = {2023-11-24},
  pmcid = {PMC1706071},
  pmid = {17492047},
  file = {/home/work/Zotero/storage/JL9QWCDY/Lewallen and Courtright - 1998 - Epidemiology in Practice Case-Control Studies.pdf}
}

@article{mathieuCoronavirusPandemicCOVID192020,
  title = {Coronavirus {{Pandemic}} ({{COVID-19}})},
  author = {Mathieu, Edouard and Ritchie, Hannah and {Rod{\'e}s-Guirao}, Lucas and Appel, Cameron and Giattino, Charlie and Hasell, Joe and Macdonald, Bobbie and Dattani, Saloni and Beltekian, Diana and {Ortiz-Ospina}, Esteban and Roser, Max},
  year = {2020},
  month = mar,
  journal = {Our World in Data},
  urldate = {2023-11-28},
  abstract = {The Oxford COVID-19 Government Response Tracker (OxCGRT) stopped updating its global database on policy responses to COVID-19 at the end of 2022},
  file = {/home/work/Zotero/storage/BAT6ZTAY/policy-responses-covid.html}
}

@techreport{mcvieThirdDataReport2021,
  title = {Third {{Data Report}} on {{Police Use}} of {{Fixed Penalty Notices}} under the {{Coronavirus Regulations}} in {{Scotland}}: {{March}} to {{December}} 2020},
  shorttitle = {Third {{Data Report}} on {{Police Use}} of {{Fixed Penalty Notices}} under the {{Coronavirus Regulations}} in {{Scotland}}},
  author = {McVie, Susan and Matthews, Ben},
  year = {2021},
  month = aug,
  urldate = {2023-11-30},
  langid = {english},
  file = {/home/work/Zotero/storage/4CLSY9ES/third-data-report-on-police-use-of-fixed-penalty-notices-under-th.html}
}

@article{merloTyrannyAveragesIndiscriminate2017,
  title = {The Tyranny of the Averages and the Indiscriminate Use of Risk Factors in Public Health: {{The}} Case of Coronary Heart Disease},
  shorttitle = {The Tyranny of the Averages and the Indiscriminate Use of Risk Factors in Public Health},
  author = {Merlo, Juan and Mulinari, Shai and Wemrell, Maria and Subramanian, {\relax SV} and Hedblad, Bo},
  year = {2017},
  month = dec,
  journal = {SSM - Population Health},
  volume = {3},
  pages = {684--698},
  issn = {2352-8273},
  doi = {10.1016/j.ssmph.2017.08.005},
  urldate = {2023-11-28},
  abstract = {Modern medicine is overwhelmed by a plethora of both established risk factors and novel biomarkers for diseases. The majority of this information is expressed by probabilistic measures of association such as the odds ratio (OR) obtained by calculating differences in average ``risk'' between exposed and unexposed groups. However, recent research demonstrates that even ORs of considerable magnitude are insufficient for assessing the ability of risk factors or biomarkers to distinguish the individuals who will develop the disease from those who will not. In regards to coronary heart disease (CHD), we already know that novel biomarkers add very little to the discriminatory accuracy (DA) of traditional risk factors. However, the value added by traditional risk factors alongside simple demographic variables such as age and sex has been the subject of less discussion. Moreover, in public health, we use the OR to calculate the population attributable fraction (PAF), although this measure fails to consider the DA of the risk factor it represents. Therefore, focusing on CHD and applying measures of DA, we re-examine the role of individual demographic characteristics, risk factors, novel biomarkers and PAFs in public health and epidemiology. In so doing, we also raise a more general criticism of the traditional risk factors' epidemiology. We investigated a cohort of 6103 men and women who participated in the baseline (1991{\textendash}1996) of the Malm{\"o} Diet and Cancer study and were followed for 18 years. We found that neither traditional risk factors nor biomarkers substantially improved the DA obtained by models considering only age and sex. We concluded that the PAF measure provided insufficient information for the planning of preventive strategies in the population. We need a better understanding of the individual heterogeneity around the averages and, thereby, a fundamental change in the way we interpret risk factors in public health and epidemiology.},
  keywords = {Coronary heart disease,Discriminatory accuracy,Individual heterogeneity,Multilevel analysis,Over-diagnosis,Overtreatment,Population attributable fraction,Risk factors},
  file = {/home/work/Zotero/storage/J6RGDMJ3/Merlo et al. - 2017 - The tyranny of the averages and the indiscriminate.pdf;/home/work/Zotero/storage/2D4HXVCZ/S2352827317300757.html}
}
